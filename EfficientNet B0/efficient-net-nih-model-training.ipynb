{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_curve, auc\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\n\nprint(\"TensorFlow Version:\", tf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:17:15.906368Z","iopub.execute_input":"2025-07-09T14:17:15.906611Z","iopub.status.idle":"2025-07-09T14:17:36.973748Z","shell.execute_reply.started":"2025-07-09T14:17:15.906585Z","shell.execute_reply":"2025-07-09T14:17:36.972676Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 14:17:22.319547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752070642.579576      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752070642.650673      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow Version: 2.18.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"IMAGE_SIZE = 224\nBATCH_SIZE_PER_REPLICA = 32\nSEED = 42\nDATA_DIR = '/kaggle/input/data'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:18:38.181903Z","iopub.execute_input":"2025-07-09T14:18:38.182486Z","iopub.status.idle":"2025-07-09T14:18:38.187004Z","shell.execute_reply.started":"2025-07-09T14:18:38.182454Z","shell.execute_reply":"2025-07-09T14:18:38.185963Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Running on TPU')\nexcept ValueError:\n    strategy = tf.distribute.MirroredStrategy()\n    print('Running on GPU(s)')\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nprint(f\"Effective batch size: {BATCH_SIZE}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:19:05.005446Z","iopub.execute_input":"2025-07-09T14:19:05.005742Z","iopub.status.idle":"2025-07-09T14:19:05.046013Z","shell.execute_reply.started":"2025-07-09T14:19:05.005719Z","shell.execute_reply":"2025-07-09T14:19:05.044883Z"}},"outputs":[{"name":"stdout","text":"Running on GPU(s)\nREPLICAS:  1\nEffective batch size: 32\n","output_type":"stream"},{"name":"stderr","text":"2025-07-09 14:19:05.009587: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\ntry:\n    df = pd.read_csv(os.path.join(DATA_DIR, 'Data_Entry_2017.csv'))\n    print(\"Metadata loaded successfully.\")\nexcept FileNotFoundError:\n    print(f\"Error: 'Data_Entry_2017.csv' not found in '{DATA_DIR}'.\")\n    print(\"Please ensure the dataset is available and the DATA_DIR path is correct.\")\n    # Exit or raise an exception if the file is not found\n    exit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:19:27.630821Z","iopub.execute_input":"2025-07-09T14:19:27.631104Z","iopub.status.idle":"2025-07-09T14:19:27.949081Z","shell.execute_reply.started":"2025-07-09T14:19:27.631083Z","shell.execute_reply":"2025-07-09T14:19:27.948305Z"}},"outputs":[{"name":"stdout","text":"Metadata loaded successfully.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Create a complete mapping of image filenames to their full paths\nall_image_paths = {os.path.basename(p): p for p in glob.glob(os.path.join(DATA_DIR, '**', '*.png'), recursive=True)}\ndf['path'] = df['Image Index'].map(all_image_paths.get)\n\n# Drop rows with no valid image path\ndf = df.dropna(subset=['path'])\nprint(f\"Found {len(df)} images with corresponding metadata.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:21:41.327539Z","iopub.execute_input":"2025-07-09T14:21:41.327863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify all unique pathology labels\nall_labels = np.unique(list(chain.from_iterable(df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n# 'No Finding' is the absence of other labels, so we treat it as the baseline\nall_labels = [label for label in all_labels if label!= 'No Finding']\nprint(f'All Labels ({len(all_labels)}): {all_labels}')\n# Create multi-hot encoded columns for each pathology\nfor label in all_labels:\n    df[label] = df['Finding Labels'].map(lambda finding: 1.0 if label in finding else 0.0)\n\nprint(\"\\nSample of processed DataFrame:\")\nprint(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npathology_counts = df[all_labels].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(18, 8))\nsns.barplot(x=pathology_counts.index, y=pathology_counts.values, palette='viridis')\nplt.title('Distribution of Pathologies in NIH Chest X-ray Dataset', fontsize=16)\nplt.ylabel('Number of Cases', fontsize=12)\nplt.xlabel('Pathology', fontsize=12)\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\nprint(\"\\nPathology Counts:\")\nprint(pathology_counts)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# %%\nfrom sklearn.model_selection import train_test_split\n\n# Extract unique patient IDs\npatient_ids = df['Patient ID'].unique()\n\n# Split patient IDs: 80% for training/validation, 20% for testing\ntrain_val_ids, test_ids = train_test_split(patient_ids, test_size=0.2, random_state=SEED)\n\n# Split the first group further: 90% for training, 10% for validation\ntrain_ids, val_ids = train_test_split(train_val_ids, test_size=0.1, random_state=SEED)\n\n# Create dataframes based on the patient ID splits\ntrain_df = df[df['Patient ID'].isin(train_ids)]\nval_df = df[df['Patient ID'].isin(val_ids)]\ntest_df = df[df['Patient ID'].isin(test_ids)]\n\n# Print summary\nprint(f\"Total Patients: {len(patient_ids)}\")\nprint(f\"Train Patients: {len(train_ids)}, Validation Patients: {len(val_ids)}, Test Patients: {len(test_ids)}\")\nprint(f\"Train Samples: {len(train_df)}, Validation Samples: {len(val_df)}, Test Samples: {len(test_df)}\")\n\n# %%\n# We calculate class weights from the TRAINING SET ONLY to avoid data leakage.\n# These weights will be used in a custom loss function to penalize errors\n# on minority classes more heavily.\n\n# Count positive and negative cases for each class in the training set\npos_counts = train_df[all_labels].sum()\nneg_counts = len(train_df) - pos_counts\n\n# Create a (num_classes, 2) tensor of weights\n# weights[i, 1] is for the positive case of class i, weights[i, 0] is for the negative\nweights = np.zeros((len(all_labels), 2))\ntotal_samples = len(train_df)\nfor i, label in enumerate(all_labels):\n    # Weight for the positive class\n    pos_weight = (1 / pos_counts[label]) * (total_samples / 2.0)\n    # Weight for the negative class\n    neg_weight = (1 / neg_counts[label]) * (total_samples / 2.0)\n    weights[i, 1] = pos_weight\n    weights[i, 0] = neg_weight\n\nprint(\"Weights calculated successfully.\")\nprint(\"Sample Positive Weights:\", {label: f\"{w:.2f}\" for label, w in zip(all_labels[:5], weights[:5, 1])})\nprint(\"Sample Negative Weights:\", {label: f\"{w:.2f}\" for label, w in zip(all_labels[:5], weights[:5, 0])})\n\n# Visualize the positive class weights\nplt.figure(figsize=(18, 8))\nsns.barplot(x=all_labels, y=weights[:, 1], palette='rocket')\nplt.title('Calculated Positive Class Weights for Each Pathology', fontsize=16)\nplt.ylabel('Weight', fontsize=12)\nplt.xlabel('Pathology', fontsize=12)\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n# %%\ndata_augmentation_pipeline = keras.Sequential([\\\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomBrightness(0.1)\n], name=\"data_augmentation\")\n\n# Visualize augmentation effects\nsample_image_path = train_df.iloc[0]['path']\nsample_image = tf.io.read_file(sample_image_path)\nsample_image = tf.image.decode_png(sample_image, channels=3)\n\nsample_image = tf.image.resize(sample_image, (IMAGE_SIZE, IMAGE_SIZE))\n\n\nplt.figure(figsize=(10, 10))\nplt.suptitle(\"Data Augmentation Examples\", fontsize=16)\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    # The data augmentation pipeline expects a batch of images.\n    # tf.expand_dims adds the batch dimension.\n    augmented_image_batch = data_augmentation_pipeline(tf.expand_dims(sample_image, 0), training=True)\n    # We remove the batch dimension to display a single image.\n    augmented_image = augmented_image_batch[0]\n    # Keras layers output float tensors. For display, we might need to cast to uint8\n    # if the values are not in the [0, 1] range. Normalizing to [0, 1] is safer.\n    plt.imshow(augmented_image / 255.0)\n    plt.axis(\"off\")\nplt.show()\n\n# %%\ndef parse_image(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image,size=[IMAGE_SIZE, IMAGE_SIZE],method=tf.image.ResizeMethod.BILINEAR,\n    preserve_aspect_ratio=False,\n    antialias=False,\n    name=None)\n\n    return image, label\n\ndef create_dataset(df, augment=False):\n    # This function is now simplified, augmentation is handled in the model\n    # Create a dataset of file paths and labels\n    dataset = tf.data.Dataset.from_tensor_slices((df['path'].values, df[all_labels].values))\n\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    # Map the parsing function\n    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n\n    # Configure for performance\n    # NOTE: The augmentation step is removed from here\n    dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    return dataset\n\n# Create the datasets\ntrain_ds = create_dataset(train_df, augment=True)\nval_ds = create_dataset(val_df, augment=False)\ntest_ds = create_dataset(test_df, augment=False)\n\nprint(\"tf.data pipelines created successfully.\")\n\n# %%\n# --- Custom Weighted Loss Function ---\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        weights_tensor = tf.constant(weights, dtype=tf.float32)\n        # The core logic: multiply the loss for each class by its corresponding weight.\n        # tf.where selects the appropriate weight based on the true label.\n        loss_weights = tf.where(tf.equal(y_true, 1.0), weights_tensor[:, 1], weights_tensor[:, 0])\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        weighted_bce = loss_weights * bce\n        return tf.reduce_mean(weighted_bce)\n    return weighted_loss\n\n# %%\nwith strategy.scope():\n    base_model = EfficientNetB0(\n        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n\n    inputs = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    x = data_augmentation_pipeline(inputs)\n    x = preprocess_input(x) # EfficientNet preprocessing\n    x = base_model(x, training=False)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    outputs = tf.keras.layers.Dense(len(all_labels), activation='sigmoid')(x)\n    model = tf.keras.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=get_weighted_loss(weights),\n        metrics=[tf.keras.metrics.AUC(name='auc_roc', multi_label=True)]\n    )\n\nmodel.summary()\n\n# %%\nprint(\"Starting initial training (feature extraction)...\")\n\n# Define callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc_roc', patience=3, mode='max', restore_best_weights=True)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'initial_training_weights.weights.h5',\n    save_weights_only=True,\n    monitor='val_auc_roc',\n    mode='max',\n    save_best_only=True\n)\n\ncheckpoint_model = tf.keras.callbacks.ModelCheckpoint(\n    'initial_training_model.keras',\n    save_weights_only=False,\n    monitor='val_auc_roc',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\nhistory = model.fit(\n    train_ds,\n    epochs=10,\n    validation_data=val_ds,\n    callbacks=[early_stopping, model_checkpoint]\n)\n\nprint(\"Initial training complete.\")\n\n# %%\ndef plot_training_history(history):\n    \"\"\"Plots training and validation loss and AUC.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\n    ax1.plot(history.history['loss'], label='Training Loss')\n    ax1.plot(history.history['val_loss'], label='Validation Loss')\n    ax1.set_title('Model Loss', fontsize=16)\n    ax1.set_ylabel('Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.legend(loc='upper right')\n\n    # Plot AUC\n    ax2.plot(history.history['auc_roc'], label='Training AUC')\n    ax2.plot(history.history['val_auc_roc'], label='Validation AUC')\n    ax2.set_title('Model AUC', fontsize=16)\n    ax2.set_ylabel('AUC')\n    ax2.set_xlabel('Epoch')\n    ax2.legend(loc='lower right')\n\n    plt.tight_layout()\n    plt.show()\n\nplot_training_history(history)\n# %%\n# Fine-tuning: unfreeze some layers of the base model and train with a lower learning rate.\nprint(\"Starting fine-tuning...\")\n\n# Unfreeze the top layers of the model\nbase_model.trainable = True\n\n# Let's unfreeze the top 20 layers.\nfor layer in base_model.layers[:-20]:\n    layer.trainable = False\n\n# Recompile the model with a lower learning rate\nwith strategy.scope():\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss=get_weighted_loss(weights),\n        metrics=[tf.keras.metrics.AUC(name='auc_roc', multi_label=True)]\n    )\n\n# Define new callbacks for fine-tuning\nfine_tune_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc_roc', patience=3, mode='max', restore_best_weights=True)\nfine_tune_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'fine_tuning_weights.weights.h5',\n    save_weights_only=True,\n    monitor='val_auc_roc',\n    mode='max',\n    save_best_only=True\n)\n\nfine_tune_checkpoint_model = tf.keras.callbacks.ModelCheckpoint(\n    'fine_tuning_model.keras',\n    save_weights_only=False,\n    monitor='val_auc_roc',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\n# Continue training\nhistory_fine_tune = model.fit(\n    train_ds,\n    epochs=10,\n    validation_data=val_ds,\n    callbacks=[fine_tune_early_stopping, fine_tune_model_checkpoint]\n)\n\nprint(\"Fine-tuning complete.\")\n\n# %%\ndef plot_fine_tuning_history(initial_history, fine_tune_history):\n    \"\"\"Plots combined training history of initial and fine-tuning phases.\"\"\"\n    acc = initial_history.history['auc_roc'] + fine_tune_history.history['auc_roc']\n    val_acc = initial_history.history['val_auc_roc'] + fine_tune_history.history['val_auc_roc']\n    loss = initial_history.history['loss'] + fine_tune_history.history['loss']\n    val_loss = initial_history.history['val_loss'] + fine_tune_history.history['val_loss']\n\n    plt.figure(figsize=(12, 12))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training AUC')\n    plt.plot(val_acc, label='Validation AUC')\n    plt.ylim([min(plt.ylim()), 1])\n    plt.plot([len(initial_history.history['auc_roc'])-1, len(initial_history.history['auc_roc'])-1],\n              plt.ylim(), label='Start Fine Tuning')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation AUC', fontsize=16)\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss', fontsize=16)\n    plt.xlabel('epoch')\n    plt.show()\n\nplot_fine_tuning_history(history, history_fine_tune)\n# %%\n# Load the best weights from fine-tuning for evaluation\nmodel.load_weights('fine_tuning_weights.weights.h5')\n\nprint(\"Evaluating on the test set...\")\ntest_loss, test_auc = model.evaluate(test_ds)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test AUC: {test_auc}\")\n# %%\n# Generate predictions for the test set\ny_pred = model.predict(test_ds)\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(np.round(test_df[all_labels].values), np.round(y_pred), target_names=all_labels))\n\n# Plot ROC curves for each pathology\nplt.figure(figsize=(15, 15))\nfor i, label in enumerate(all_labels):\n    fpr, tpr, _ = roc_curve(test_df[all_labels].values[:, i], y_pred[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves for Each Pathology')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}