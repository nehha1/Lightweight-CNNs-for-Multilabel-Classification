{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12240135,"sourceType":"datasetVersion","datasetId":7712322},{"sourceId":247727436,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport cv2 # OpenCV for image processing\n\nprint(\"TensorFlow Version:\", tf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_INPUT_PATH = '/kaggle/input/downsized-data'\nMODEL_PATH = '/kaggle/input/nih-phase-4-further-fine-tuning/best_further_fine_tuned_model.keras'\nIMAGE_DIR = os.path.join(BASE_INPUT_PATH, 'nih_images_resized_256/')\nCSV_PATH = os.path.join(BASE_INPUT_PATH, 'Data_Entry_2017_stratified_downsized.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE=224\nBATCH_SIZE=32\nAUTOTUNE= tf.data.AUTOTUNE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FINAL_14_LABELS = [\n    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n    'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule',\n    'Pneumonia', 'Pneumothorax', 'No Finding'\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Preparing Validation Data ---\")\ntry:\n    df = pd.read_csv(CSV_PATH)\n\n    # One-hot encode the labels\n    for label in FINAL_14_LABELS:\n        df[label] = df['Finding Labels'].str.lower().apply(\n            lambda finding_string: 1 if label.lower() in str(finding_string) else 0\n        )\n    df['image_path'] = df['Image Index'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n    _,val_df = train_test_split(df, test_size=0.15, random_state=42)\n\n    print(f\"Validation set size: {len(val_df)} images\")\n\n    # --- TensorFlow Dataset Pipeline ---\n    def parse_function(image_path, labels):\n        \"\"\"Loads and preprocesses an image.\"\"\"\n        img_raw = tf.io.read_file(image_path)\n        img = tf.io.decode_png(img_raw, channels=1)\n        img = tf.image.convert_image_dtype(img, tf.float32)\n        img = tf.image.grayscale_to_rgb(img) # Model expects 3 channels\n        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n        return img, labels\n\n    def create_dataset(dataframe):\n        \"\"\"Creates a tf.data.Dataset from a pandas DataFrame.\"\"\"\n        image_paths = dataframe['image_path'].values\n        labels = dataframe[FINAL_14_LABELS].values.astype(np.float32)\n        ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n        ds = ds.map(parse_function, num_parallel_calls=AUTOTUNE)\n        return ds\n\n    # Create the validation dataset\n    validation_dataset = (\n        create_dataset(val_df)\n        .batch(BATCH_SIZE)\n        .prefetch(buffer_size=AUTOTUNE)\n    )\n\n    print(\"\\nComplete: `validation_dataset` is ready.\")\n    \n    # Clean up to free memory\n    del df\n    gc.collect()\n\nexcept FileNotFoundError:\n    print(\"Error: Data files not found. Please ensure the CSV and image directories are correctly specified.\")\n    validation_dataset = None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"--- Loading Model from: {MODEL_PATH} ---\")\ntry:\n    model = tf.keras.models.load_model(MODEL_PATH)\n    model.summary()\n    print(\"\\nModel loaded successfully.\")\nexcept (IOError, FileNotFoundError) as e:\n    print(f\"Error loading model: {e}\")\n    print(\"Please ensure the model path is correct and the file is available.\")\n    model = None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if model and validation_dataset:\n    print(\"--- Evaluating Model Performance on Validation Data ---\")\n    \n    y_true = np.concatenate([y for x, y in validation_dataset], axis=0)\n    print(\"Generating predictions...\")\n    y_pred_probs = model.predict(validation_dataset, verbose=1)\n    y_pred_binary = (y_pred_probs > 0.5).astype(int)\n\n    # --- Classification Report ---\n    # This report shows precision, recall, and F1-score for each class.\n    print(\"\\n\\n--- Classification Report ---\")\n    report = classification_report(y_true, y_pred_binary, target_names=FINAL_14_LABELS)\n    print(report)\n\n    # --- Confusion Matrices ---\n    # For multi-label classification\n    print(\"\\n--- Plotting Confusion Matrices for Each Label ---\")\n    mcm = multilabel_confusion_matrix(y_true, y_pred_binary)\n    \n    fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n    axes = axes.flatten()\n    \n    # Remove the last two subplots as we only have 14 labels\n    fig.delaxes(axes[14])\n    fig.delaxes(axes[15])\n\n    for i, (matrix, label) in enumerate(zip(mcm, FINAL_14_LABELS)):\n        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n                    xticklabels=['Predicted Negative', 'Predicted Positive'],\n                    yticklabels=['Actual Negative', 'Actual Positive'],\n                    cbar=False)\n        axes[i].set_title(f'Confusion Matrix: {label}', fontsize=12)\n    \n    plt.suptitle('Confusion Matrix for Each Pathology (Validation Set)', fontsize=20, y=1.02)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Model or validation data not available. Skipping evaluation.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}